{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49ad1c23",
   "metadata": {},
   "source": [
    "# Chapter 3 Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edec6822",
   "metadata": {},
   "source": [
    "## Conceptual"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75107fd1",
   "metadata": {},
   "source": [
    "1. The $p$-values correspond to the null hypothesis that each factor has no effect on the regression model. More precisely, the null hypothesis is that the coefficient is $=0$.\n",
    "2. The KNN classifier attempts to predict a categorical response variable, and so it can only return a discrete set of values. It always returns the value that is most represented among the $k$ nearest neighbors. On the other hand, the KNN regression method *averages* over the $k$ nearest neighbors, because it is predicting a continuous-valued response variable.\n",
    "3. (a) Answer iii. is correct. If GPA > 3.5, then $\\beta_3 + \\beta_5 X_1 < 0$ and males earn more. (b) \\\\$137,100 per year (c) False, whether the interaction is significant or not depends on the $p$-value of that coefficient\n",
    "4. (a) We would expect the cubic regression to have a smaller training RSS. The linear model is a subset of the cubic model, and so in general the cubic model should be able to fit the training data at least as well as the linear model. (b) We would not expect the cubic regression to have a smaller test RSS. The cubic model is more flexible than the linear one, and so we would expect its variance to be higher. However because the true relationship is linear, the bias of the cubic model won't be any smaller than the linear one. (c) Even if we don't know the true relationship of $X$ and $Y$, we would still expect the cubic model to have a smaller training RSS than the linear model, for the same reason as stated in (a). (d) There is not enough information to know if the test RSS would be smaller for the cubic or linear model. If the true relationship is *close* to linear, the test RSS may still be smaller for the linear model, but if the true relationship is nonlinear enough, the cubic model will have the smaller test RSS.\n",
    "5. $\\hat{y}_i = x_i \\hat{\\beta} = x_i \\frac{\\sum_{j=1}^N x_j y_j}{\\sum_{i'=1}^N x_{i'}^2} = \\sum_{i'=1}^N \\left(\\frac{x_i x_{i'}}{\\sum_{j=1}^N x_j^2} \\right) y_{i'}$\n",
    "6. A simple rearrangement of equation 3.4 gives $y - \\bar{y} = \\hat{\\beta}_1 (x - \\bar{x})$ which is the equation for a line with slope $\\hat{\\beta}_1$ through the point $(\\bar{x}, \\bar{y})$\n",
    "7. Assuming $\\bar{x} = \\bar{y} = 0$, then\n",
    "$$\\text{RSS} = \\frac{\\sum_i y_i^2 - \\sum_i (y_i - \\hat{y}_i)^2}{\\sum_i y_i^2}.$$ From the formula for simple linear regression, $$\\hat{y}_i = \\left(\\frac{\\sum_j x_j y_j}{\\sum_j x_j^2}\\right) x_i.$$ Substituting this into the previous equation gives $$\\text{RSS} = \\frac{\\sum_i y_i^2 - \\sum_i \\left(y_i^2 - 2 \\left(\\frac{\\sum_j x_j y_j}{\\sum_j x_j^2}\\right) x_i y_i + \\left(\\frac{\\sum_j x_j y_j}{\\sum_j x_j^2}\\right)^2 x_i^2\\right)}{\\sum_i y_i^2}.$$ The lone $\\sum_i y_i^2$ terms in the numerator cancel, and all of the sums over $j$ can be pulled out of the sums with $i$: $$= \\frac{2\\left(\\frac{\\sum_i x_i y_i}{\\sum_i x_i^2}\\right) \\left(\\sum_i x_i y_i\\right) - \\left(\\frac{\\sum_i x_i y_i}{\\sum_i x_i^2}\\right)^2 \\left(\\sum_i x_i\\right)^2}{\\sum_i y_i^2}.$$ Finally, simplifying what is left: $$\\text{RSS} = \\frac{\\left(\\sum_i x_i y_i\\right)^2}{\\left(\\sum_i x_i^2\\right) \\left(\\sum_i y_i^2\\right)} = \\left(\\text{Cor}(X, Y)\\right)^2.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94852d1b",
   "metadata": {},
   "source": [
    "## Applied"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7666fc74",
   "metadata": {},
   "source": [
    "Import the necessary python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "48357dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from patsy import dmatrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f9a5d937",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520229b6",
   "metadata": {},
   "source": [
    "8. Linear regression on the `Auto` data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bbfbaa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "auto = sm.datasets.get_rdataset('Auto', 'ISLR')\n",
    "auto_df = auto.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "68854f78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            OLS Regression Results                            \n",
      "==============================================================================\n",
      "Dep. Variable:                    mpg   R-squared:                       0.606\n",
      "Model:                            OLS   Adj. R-squared:                  0.605\n",
      "Method:                 Least Squares   F-statistic:                     599.7\n",
      "Date:                Sat, 10 Jul 2021   Prob (F-statistic):           7.03e-81\n",
      "Time:                        12:58:52   Log-Likelihood:                -1178.7\n",
      "No. Observations:                 392   AIC:                             2361.\n",
      "Df Residuals:                     390   BIC:                             2369.\n",
      "Df Model:                           1                                         \n",
      "Covariance Type:            nonrobust                                         \n",
      "==============================================================================\n",
      "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
      "------------------------------------------------------------------------------\n",
      "Intercept     39.9359      0.717     55.660      0.000      38.525      41.347\n",
      "horsepower    -0.1578      0.006    -24.489      0.000      -0.171      -0.145\n",
      "==============================================================================\n",
      "Omnibus:                       16.432   Durbin-Watson:                   0.920\n",
      "Prob(Omnibus):                  0.000   Jarque-Bera (JB):               17.305\n",
      "Skew:                           0.492   Prob(JB):                     0.000175\n",
      "Kurtosis:                       3.299   Cond. No.                         322.\n",
      "==============================================================================\n",
      "\n",
      "Notes:\n",
      "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n"
     ]
    }
   ],
   "source": [
    "y, X = dmatrices('mpg ~ horsepower', data=auto_df, return_type='dataframe')\n",
    "auto_mod = sm.OLS(y, X)\n",
    "auto_res = auto_mod.fit()\n",
    "print(auto_res.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5733c0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: 24.467077152512424\n",
      "95% Confidence interval: [23.97307896070394, 24.961075344320907]\n",
      "95% Prediction interval: [14.809396070967116, 34.12475823405773]\n"
     ]
    }
   ],
   "source": [
    "prediction = auto_res.get_prediction([1, 98])\n",
    "mu, sig2r, sig2e = prediction.predicted_mean[0], prediction.var_resid, prediction.var_pred_mean[0]\n",
    "coef = prediction.dist.ppf(df=prediction.df, q=0.975)\n",
    "print('Prediction: {}'.format(mu))\n",
    "print('95% Confidence interval: [{}, {}]'.format(*list(prediction.conf_int()[0])))\n",
    "print('95% Prediction interval: [{}, {}]'.format(\n",
    "    mu - coef * np.sqrt(sig2r + sig2e), mu + coef * np.sqrt(sig2r + sig2e)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74a7f94a",
   "metadata": {},
   "source": [
    "(a) There is a significant negative relationship between horsepower and mpg. The predicted mpg is 24.5 mpg, with 95% confidence and prediction intervals as given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e2a418",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769d89c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
